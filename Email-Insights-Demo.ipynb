{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Insights from Email - First Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/dave-email.png\" width=\"800\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing - Retrieving the Email Data\n",
    "\n",
    "Let's start by getting the email data so we can analyse it.\n",
    "\n",
    "At this point in the process we have received the email from microsoft\n",
    "(https://docs.microsoft.com/en-us/graph/api/resources/mail-api-overview?view=graph-rest-1.0) and have saved it into an AWS S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/step-function-preprocess.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our pre-processing Lambda the first thing we would do is read the email from S3.\n",
    "Here we will pull it in from the local file system, but the file is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data/dave-email.json', 'r') as email_file:\n",
    "    data=email_file.read()\n",
    "email_json = json.loads(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file we get from the Graph API is a json structure.\n",
    "\n",
    "<img src=\"img/dave-email-json.png\" width=\"800\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the data what should we do with it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#61A794>QUESTION: What information do YOU use to understand an email?</font>\n",
    "\n",
    "Here we are first going to look at the meta data attached to the email (sender, date, etc) and do some light formatting on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "META-DATA\n",
      "Subject:  Hey there!\n",
      "Sent:  2020-10-30T09:27:00Z\n",
      "Sender:  David McFadden\n",
      "Recipients:  Kevin Duffy\n"
     ]
    }
   ],
   "source": [
    "import probablepeople\n",
    "\n",
    "email_subject = email_json['subject']\n",
    "email_sent = email_json['sentDateTime']\n",
    "orig_email_sender = email_json['sender']['emailAddress']['name']\n",
    "parsed_sender = probablepeople.tag(orig_email_sender)\n",
    "email_sender = parsed_sender[0]['GivenName'] +' ' + parsed_sender[0]['Surname']\n",
    "\n",
    "email_to_recipient_array = email_json['toRecipients']\n",
    "email_recipients = ''\n",
    "\n",
    "for index, value in enumerate(email_to_recipient_array):\n",
    "    if index > 0:\n",
    "        email_recipients = email_recipients + ' and '\n",
    "    name = value['emailAddress']['name']\n",
    "    parsed_name = probablepeople.tag(name)\n",
    "    email_recipients = email_recipients + parsed_name[0]['GivenName'] +' ' + parsed_name[0]['Surname']\n",
    "    \n",
    "\n",
    "def print_meta_data():\n",
    "    print('META-DATA')\n",
    "    print('Subject: ', email_subject)\n",
    "    print('Sent: ', email_sent)\n",
    "    print('Sender: ', email_sender)\n",
    "    print('Recipients: ', email_recipients)\n",
    "\n",
    "print_meta_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html><head>\\r\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"><meta content=\"text/html; charset=utf-8\"><meta name=\"Generator\" content=\"Microsoft Word 15 (filtered medium)\"><style>\\r\\n<!--\\r\\n@font-face\\r\\n\\t{font-family:Wingdings}\\r\\n@font-face\\r\\n\\t{font-family:\"Cambria Math\"}\\r\\n@font-face\\r\\n\\t{font-family:Calibri}\\r\\n@font-face\\r\\n\\t{}\\r\\np.MsoNormal, li.MsoNormal, div.MsoNormal\\r\\n\\t{margin:0cm;\\r\\n\\tfont-size:12.0pt;\\r\\n\\tfont-family:\"Calibri\",sans-serif}\\r\\na:link, span.MsoHyperlink\\r\\n\\t{color:#0563C1;\\r\\n\\ttext-decoration:underline}\\r\\np.MsoListParagraph, li.MsoListParagraph, div.MsoListParagraph\\r\\n\\t{margin-top:0cm;\\r\\n\\tmargin-right:0cm;\\r\\n\\tmargin-bottom:0cm;\\r\\n\\tmargin-left:36.0pt;\\r\\n\\tfont-size:12.0pt;\\r\\n\\tfont-family:\"Calibri\",sans-serif}\\r\\nspan.EmailStyle20\\r\\n\\t{font-family:\"Calibri\",sans-serif;\\r\\n\\tcolor:windowtext}\\r\\n.MsoChpDefault\\r\\n\\t{font-size:10.0pt}\\r\\n@page WordSection1\\r\\n\\t{margin:72.0pt 72.0pt 72.0pt 72.0pt}\\r\\ndiv.WordSection1\\r\\n\\t{}\\r\\nol\\r\\n\\t{margin-bottom:0cm}\\r\\nul\\r\\n\\t{margin-bottom:0cm}\\r\\n-->\\r\\n</style></head><body lang=\"EN-GB\" link=\"#0563C1\" vlink=\"purple\" style=\"word-wrap:break-word\"><div class=\"WordSection1\"><div><p class=\"MsoNormal\"><span style=\"font-size:11.0pt\">Hey Kevin!</span></p></div><p class=\"MsoNormal\"><span style=\"font-size:11.0pt\">&nbsp;</span></p><p class=\"MsoNormal\"><span style=\"font-size:11.0pt\">How are you? It’s been a while since we’ve chatted.</span></p><p class=\"MsoNormal\">&nbsp;</p><p class=\"MsoNormal\"><span style=\"font-size:11.0pt\">I was wondering if you had time to meet with Gillian and me next Wednesday at <b>3pm</b>?</span></p><p class=\"MsoNormal\">&nbsp;</p><p class=\"MsoNormal\"><span style=\"font-size:11.0pt\">I also was also hoping you could take a look over the following documents:</span></p><ul type=\"disc\" style=\"margin-top:0cm\"><li class=\"MsoListParagraph\" style=\"margin-left:0cm; mso-list:l1 level1 lfo3\"><span style=\"font-size:11.0pt\">Amazing Flow Charts</span></li><li class=\"MsoListParagraph\" style=\"margin-left:0cm; mso-list:l1 level1 lfo3\"><span style=\"font-size:11.0pt\">Getting Fancy with AWS Step Functions</span></li><li class=\"MsoListParagraph\" style=\"margin-left:0cm; mso-list:l1 level1 lfo3\"><span style=\"font-size:11.0pt\">Prototyping with Buttery Services</span></li></ul><p class=\"MsoNormal\"><span style=\"font-size:11.0pt\">&nbsp;</span></p><p class=\"MsoNormal\"><span style=\"font-size:11.0pt\">I’ll need any comments/questions/compliments back by Monday.</span></p><p class=\"MsoNormal\"><span style=\"font-size:11.0pt\">&nbsp;</span></p><p class=\"MsoNormal\"><span style=\"font-size:11.0pt\">You can find out more about the Amazon AI Services <a href=\"https://aws.amazon.com/machine-learning/ai-services/\">here</a>.</span></p><p class=\"MsoNormal\"><span style=\"font-size:11.0pt\">&nbsp;</span></p><p class=\"MsoNormal\"><span style=\"font-size:11.0pt\">Cheers,</span></p><p class=\"MsoNormal\"><span style=\"font-size:11.0pt\">Dave</span></p><p class=\"MsoNormal\">&nbsp;</p><p class=\"MsoNormal\" style=\"line-height:105%\"><b><span style=\"font-size:10.0pt; line-height:105%; font-family:&quot;Arial&quot;,sans-serif\">David McFadden<span style=\"color:#00006D\"> |&nbsp;</span></span></b><span style=\"font-size:10.0pt; line-height:105%; font-family:&quot;Arial&quot;,sans-serif\">Developer</span></p><p class=\"MsoNormal\"><b><span style=\"font-size:10.0pt; font-family:&quot;Arial&quot;,sans-serif; color:#6D6D6D\">Example Corp. </span></b></p><p class=\"MsoNormal\" style=\"line-height:14.0pt\"><span style=\"font-size:10.0pt; font-family:&quot;Arial&quot;,sans-serif; color:#6D6D6D\">1007 Main Street, Belfast</span></p><p class=\"MsoNormal\" style=\"line-height:14.0pt\"><span style=\"font-size:10.0pt; font-family:&quot;Arial&quot;,sans-serif; color:#6D6D6D\">Tel: +44 2896496000</span>&nbsp;</p><p class=\"MsoNormal\" style=\"line-height:12.6pt\"><span style=\"font-size:10.0pt; font-family:&quot;Arial&quot;,sans-serif; color:gray\">&nbsp;</span></p><p class=\"MsoNormal\"><i><span style=\"font-size:9.0pt; font-family:&quot;Arial&quot;,sans-serif; color:#616265\">Disclaimer: The contents of this e-mail and attached files in no way reflect any policies of Example Corp or their affiliated superhero counterparts.</span></i></p><p class=\"MsoNormal\">&nbsp;</p></div></body></html>'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_body = email_json['body']['content']\n",
    "email_body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing - Cleaning up the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's strip the HTML. There are lots of different libraries available for this, or you can write your pre-preprocessing method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Processing: 4177 characters\n",
      "After Processing: 676 characters\n",
      "Now it's only 16.18% the size of the original!\n",
      "\n",
      "\n",
      "Hey Kevin! How are you? It’s been a while since we’ve chatted. I was wondering if you had time to meet with Gillian and me next Wednesday at 3pm? I also was also hoping you could take a look over the following documents:Amazing Flow ChartsGetting Fancy with AWS Step FunctionsPrototyping with Buttery Services I’ll need any comments/questions/compliments back by Monday. You can find out more about the Amazon AI Services here. Cheers,Dave David McFadden | DeveloperExample Corp. 1007 Main Street, BelfastTel: +44 2896496000  Disclaimer: The contents of this e-mail and attached files in no way reflect any policies of Example Corp or their affiliated superhero counterparts. \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "#strip out the header contents - in this case it's just meta-data on formatting\n",
    "email_body_only = re.sub(r'<head>(\\r|\\n|.)*</head>', '', email_body)\n",
    "parsed_email_body_1 = BeautifulSoup(email_body_only, features=\"html.parser\")\n",
    "parsed_email_text_1 = parsed_email_body_1.get_text()\n",
    "\n",
    "print(f\"Before Processing: {len(email_body)} characters\")\n",
    "print(f\"After Processing: {len(parsed_email_text_1)} characters\")\n",
    "print(f\"Now it's only {round(len(parsed_email_text_1)/len(email_body) * 100, 2)}% the size of the original!\")\n",
    "\n",
    "print('')\n",
    "print('')\n",
    "print(parsed_email_text_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/dave-email.png\" width=\"600\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey Kevin!\n",
      " \n",
      "How are you? It’s been a while since we’ve chatted.\n",
      " \n",
      "I was wondering if you had time to meet with Gillian and me next Wednesday at 3pm?\n",
      " \n",
      "I also was also hoping you could take a look over the following documents:\n",
      "Amazing Flow Charts\n",
      "Getting Fancy with AWS Step Functions\n",
      "Prototyping with Buttery Services\n",
      " \n",
      "I’ll need any comments/questions/compliments back by Monday.\n",
      " \n",
      "You can find out more about the Amazon AI Services here.\n",
      " \n",
      "Cheers,\n",
      "Dave\n",
      " \n",
      "David McFadden | Developer\n",
      "Example Corp. \n",
      "1007 Main Street, Belfast\n",
      "Tel: +44 2896496000 \n",
      " \n",
      "Disclaimer: The contents of this e-mail and attached files in no way reflect any policies of Example Corp or their affiliated superhero counterparts.\n",
      " \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pre_processed_email = email_body_only\n",
    "# As you explore more data you will find other things you want to preserve in your text\n",
    "pre_processed_email = pre_processed_email.replace('</div>', '\\n')\n",
    "pre_processed_email = pre_processed_email.replace('</li>', '\\n')\n",
    "pre_processed_email = pre_processed_email.replace('</p>', '\\n')\n",
    "\n",
    "parsed_email = BeautifulSoup(pre_processed_email, features=\"html.parser\")\n",
    "parsed_email_text = parsed_email.get_text()\n",
    "\n",
    "print(parsed_email_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|￣￣￣￣￣￣￣ |  \n",
    "|&nbsp;FORMATTING &nbsp;|  \n",
    "|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;IS DATA &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|  \n",
    "|＿＿＿＿＿＿＿ |  \n",
    "(\\\\__/) ||  \n",
    "(•ㅅ•) ||  \n",
    "/ 　 づ  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE!** For your use case the HTML may be important, make sure you have examined what information is being used to understand it at at the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Nice! Let's move on to the Machine Learning!!_\n",
    "\n",
    "## Information Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#61A794>QUESTION: How can we identify some key information easily?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/step-function-extract.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Amazon Comprehend?\n",
    "Amazon Comprehend is a Natural Language Processing (NLP) service.\n",
    "<img src=\"img/comprehend.png\" width=\"800\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entity Extraction\n",
    "The first service we are going to use is an out-of-the-box option \"Detect Entities\". This is a pre-trained model that can recognise common references such as Dates or Locations. This is a managed service, so you don't need to set anything up.\n",
    "\n",
    "The items it current recognises are the following:\n",
    "\n",
    "| Type      | Description |\n",
    "| :- | :- |\n",
    "| COMMERCIAL_ITEM      | A branded product       |\n",
    "| DATE   | A full date (for example, 11/25/2017), day (Tuesday), month (May), or time (8:30 a.m.)        |\n",
    "| EVENT   | An event, such as a festival, concert, election, etc.        |\n",
    "| LOCATION   | A specific location, such as a country, city, lake, building, etc.        |\n",
    "| ORGANIZATION   | Large organizations, such as a government, company, religion, sports team, etc.        |\n",
    "| PERSON   | Individuals, groups of people, nicknames, fictional characters        |\n",
    "| QUANTITY   | A quantified amount, such as currency, percentages, numbers, bytes, etc.        |\n",
    "| TITLE   | An official name given to any creation or creative work, such as movies, books, songs, etc.        |\n",
    "| OTHER   | Entities that don't fit into any of the other entity categories        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Comprehend offers a full set of APIs and SDKS in a variety of languages are to make interacting with the service programatically very easy. \n",
    "\n",
    "Here we are using the python library https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/comprehend.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "comprehend = boto3.client('comprehend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Entities\": [\n",
      "    {\n",
      "      \"Score\": 0.999258816242218,\n",
      "      \"Type\": \"PERSON\",\n",
      "      \"Text\": \"Kevin\",\n",
      "      \"BeginOffset\": 4,\n",
      "      \"EndOffset\": 9\n",
      "    },\n",
      "    {\n",
      "      \"Score\": 0.9997270703315735,\n",
      "      \"Type\": \"PERSON\",\n",
      "      \"Text\": \"Gillian\",\n",
      "      \"BeginOffset\": 112,\n",
      "      \"EndOffset\": 119\n",
      "    },\n",
      "    {\n",
      "      \"Score\": 0.9064348340034485,\n",
      "      \"Type\": \"DATE\",\n",
      "      \"Text\": \"next Wednesday at 3pm\",\n",
      "      \"BeginOffset\": 127,\n",
      "      \"EndOffset\": 148\n",
      "    },\n",
      "    {\n",
      "      \"Score\": 0.993148148059845,\n",
      "      \"Type\": \"ORGANIZATION\",\n",
      "      \"Text\": \"AWS\",\n",
      "      \"BeginOffset\": 266,\n",
      "      \"EndOffset\": 269\n",
      "    },\n",
      "    {\n",
      "      \"Score\": 0.99654620885849,\n",
      "      \"Type\": \"DATE\",\n",
      "      \"Text\": \"Monday\",\n",
      "      \"BeginOffset\": 374,\n",
      "      \"EndOffset\": 380\n",
      "    },\n",
      "    {\n",
      "      \"Score\": 0.9585748910903931,\n",
      "      \"Type\": \"ORGANIZATION\",\n",
      "      \"Text\": \"Amazon AI Services\",\n",
      "      \"BeginOffset\": 416,\n",
      "      \"EndOffset\": 434\n",
      "    },\n",
      "    {\n",
      "      \"Score\": 0.9948257803916931,\n",
      "      \"Type\": \"PERSON\",\n",
      "      \"Text\": \"Dave\\n\\u00a0\\nDavid McFadden\",\n",
      "      \"BeginOffset\": 451,\n",
      "      \"EndOffset\": 472\n",
      "    },\n",
      "    {\n",
      "      \"Score\": 0.9678810238838196,\n",
      "      \"Type\": \"ORGANIZATION\",\n",
      "      \"Text\": \"Example Corp.\",\n",
      "      \"BeginOffset\": 485,\n",
      "      \"EndOffset\": 498\n",
      "    },\n",
      "    {\n",
      "      \"Score\": 0.9892089366912842,\n",
      "      \"Type\": \"LOCATION\",\n",
      "      \"Text\": \"1007 Main Street, Belfast\",\n",
      "      \"BeginOffset\": 500,\n",
      "      \"EndOffset\": 525\n",
      "    },\n",
      "    {\n",
      "      \"Score\": 0.9961594343185425,\n",
      "      \"Type\": \"OTHER\",\n",
      "      \"Text\": \"+44 2896496000\",\n",
      "      \"BeginOffset\": 531,\n",
      "      \"EndOffset\": 545\n",
      "    },\n",
      "    {\n",
      "      \"Score\": 0.992797315120697,\n",
      "      \"Type\": \"ORGANIZATION\",\n",
      "      \"Text\": \"Example Corp\",\n",
      "      \"BeginOffset\": 642,\n",
      "      \"EndOffset\": 654\n",
      "    }\n",
      "  ],\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"5502c07e-b06f-40d4-8dba-d4170d126bb2\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"x-amzn-requestid\": \"5502c07e-b06f-40d4-8dba-d4170d126bb2\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"1153\",\n",
      "      \"date\": \"Sat, 31 Oct 2020 17:55:04 GMT\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "### This is all the code needed to call Comprehend Detect Entities - no other setup required!\n",
    "comprehend_entity_response = comprehend.detect_entities(\n",
    "    Text=parsed_email_text, \n",
    "    LanguageCode='en')\n",
    "\n",
    "print(json.dumps(comprehend_entity_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each entity the service brings back the **Type**, the exact **Text** it matched, the location (**BeginOffset, EndOffset**)  of that Text in the text sent to the service and the **Score** which represents how confident Amazon Comprehend is in the accuracy of the result.\n",
    "\n",
    "Let's print it out in a way that is a little easier to look at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score / Type : Text\n",
      "--------------------\n",
      "1.0 / PERSON : Kevin\n",
      "1.0 / PERSON : Gillian\n",
      "0.91 / DATE : next Wednesday at 3pm\n",
      "0.99 / ORGANIZATION : AWS\n",
      "1.0 / DATE : Monday\n",
      "0.96 / ORGANIZATION : Amazon AI Services\n",
      "0.99 / PERSON : Dave\n",
      " \n",
      "David McFadden\n",
      "0.97 / ORGANIZATION : Example Corp.\n",
      "0.99 / LOCATION : 1007 Main Street, Belfast\n",
      "1.0 / OTHER : +44 2896496000\n",
      "0.99 / ORGANIZATION : Example Corp\n"
     ]
    }
   ],
   "source": [
    "def print_entities_compressed(comprehend_entity_response):\n",
    "    print(f\"Score / Type : Text\")\n",
    "    print('--------------------')\n",
    "    for entity in comprehend_entity_response['Entities']:\n",
    "        print(f\"{round(entity['Score'], 2)} / {entity['Type']} : {entity['Text']}\")\n",
    "              \n",
    "print_entities_compressed(comprehend_entity_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Recognition on the original HTML\n",
      "--------------------\n",
      "Score / Type : Text\n",
      "--------------------\n",
      "0.71 / OTHER : =utf-8\n",
      "0.62 / OTHER : utf-8\n",
      "0.88 / TITLE : Microsoft Word 15\n",
      "0.42 / TITLE : font-face\n",
      "0.46 / ORGANIZATION : @font-face\n",
      "0.41 / ORGANIZATION : font-face\n",
      "0.48 / TITLE : font-face\n",
      "0.97 / QUANTITY : 0cm\n",
      "0.95 / QUANTITY : 12.0pt\n",
      "1.0 / QUANTITY : 0cm\n",
      "1.0 / QUANTITY : 0cm\n",
      "1.0 / QUANTITY : 0cm\n",
      "1.0 / QUANTITY : 36.0pt\n",
      "1.0 / QUANTITY : 12.0pt\n",
      "0.98 / QUANTITY : 10.0pt\n",
      "0.99 / QUANTITY : 72.0pt\n",
      "0.98 / QUANTITY : 72.0pt\n",
      "0.98 / QUANTITY : 72.0pt\n",
      "0.96 / QUANTITY : 72.0pt\n",
      "0.93 / QUANTITY : 0cm\n",
      "0.92 / QUANTITY : 0cm\n",
      "0.8 / TITLE : WordSection1\n",
      "0.75 / QUANTITY : 11.0pt\n",
      "0.98 / PERSON : Kevin\n",
      "0.85 / QUANTITY : 11.0pt\n",
      "0.81 / QUANTITY : 11.0pt\n",
      "0.51 / QUANTITY : 11.0pt\n",
      "1.0 / PERSON : Gillian\n",
      "0.96 / DATE : next Wednesday\n",
      "0.97 / DATE : 3pm\n",
      "0.93 / QUANTITY : 11.0pt\n",
      "0.67 / QUANTITY : 0cm\n",
      "0.96 / QUANTITY : 0cm\n",
      "0.91 / QUANTITY : 11.0pt\n",
      "0.66 / QUANTITY : 0cm\n",
      "0.95 / QUANTITY : 11.0pt\n",
      "0.77 / ORGANIZATION : AWS\n",
      "0.88 / QUANTITY : 0cm\n",
      "0.67 / OTHER : level1 lfo3\n",
      "0.95 / QUANTITY : 11.0pt\n",
      "0.63 / TITLE : Buttery\n",
      "0.93 / QUANTITY : 11.0pt\n",
      "0.94 / QUANTITY : 11.0pt\n",
      "0.99 / DATE : Monday\n",
      "0.92 / QUANTITY : 11.0pt\n",
      "0.97 / QUANTITY : 11.0pt\n",
      "0.86 / ORGANIZATION : Amazon\n",
      "0.54 / TITLE : AI Services\n",
      "0.9 / OTHER : https://aws.amazon.com/machine-learning/ai-services/\">here</a>.</span></p><p\n",
      "0.41 / OTHER : MsoNormal\n",
      "0.96 / QUANTITY : 11.0pt\n",
      "0.94 / QUANTITY : 11.0pt\n",
      "0.96 / QUANTITY : 11.0pt\n",
      "0.93 / PERSON : Dave\n",
      "0.93 / QUANTITY : 105%\n",
      "0.98 / QUANTITY : 10.0pt\n",
      "0.96 / QUANTITY : 105%\n",
      "1.0 / PERSON : David McFadden\n",
      "0.99 / QUANTITY : 10.0pt\n",
      "0.99 / QUANTITY : 105%\n",
      "0.97 / QUANTITY : 10.0pt\n",
      "0.96 / QUANTITY : 14.0pt\n",
      "0.99 / QUANTITY : 10.0pt\n",
      "0.73 / OTHER : #6D6D6D\n",
      "0.95 / LOCATION : 1007 Main Street, Belfast\n",
      "0.97 / QUANTITY : 14.0pt\n",
      "0.96 / QUANTITY : 10.0pt\n",
      "0.5 / OTHER : #6D6D6D\n",
      "0.99 / OTHER : +44 2896496000\n",
      "0.93 / QUANTITY : 12.6pt\n",
      "0.97 / QUANTITY : 10.0pt\n",
      "0.76 / OTHER : #616265\n",
      "0.99 / ORGANIZATION : Example Corp\n"
     ]
    }
   ],
   "source": [
    "# cutting off the html since comprehend can only take 5,000 bytes \n",
    "# and ignoring the rest since this is just for demonstration purposes\n",
    "comprehend_entity_response_unprocessed = comprehend.detect_entities(\n",
    "    Text=str(email_body[:4800]), \n",
    "    LanguageCode='en')\n",
    "\n",
    "\n",
    "print('Entity Recognition on the original HTML')\n",
    "print('--------------------')\n",
    "print_entities_compressed(comprehend_entity_response_unprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pricing for Comprehend Entity Recognition\n",
    "$0.0001 / unit (100 chars)\n",
    "\n",
    "- Original HTML Email, 4177 characters, 42 Units = $0.0042\n",
    "\n",
    "- Processed Email, 676 characters, 7 Units = $0.0007\n",
    "\n",
    "(NOTE: Free Tier - 50K units/month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comprehend Sentiment Detection\n",
    "\n",
    "\n",
    "The second service we are going to use is another out-of-the-box option \"Detect Sentiment\". This is a pre-trained model that can recognise sentiment in text. This is a managed service, so you don't need to set anything up.\n",
    "\n",
    "The sentiments it could return are: POSITIVE, NEUTRAL, MIXED, or NEGATIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sentiment': 'NEUTRAL',\n",
       " 'SentimentScore': {'Positive': 0.34901344776153564,\n",
       "  'Negative': 0.0016374826664105058,\n",
       "  'Neutral': 0.6493454575538635,\n",
       "  'Mixed': 3.640495151557843e-06},\n",
       " 'ResponseMetadata': {'RequestId': 'f05a4964-8af3-48f7-9575-d71a1d4303d0',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'f05a4964-8af3-48f7-9575-d71a1d4303d0',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '164',\n",
       "   'date': 'Sat, 31 Oct 2020 17:56:10 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### This is all the code needed to call Comprehend Detect Sentiment - no other setup required\n",
    "\n",
    "comprehend_sentiment_response = comprehend.detect_sentiment(\n",
    "    Text=str(parsed_email_text), \n",
    "    LanguageCode='en')\n",
    "\n",
    "def print_sentiment():\n",
    "    print('SENTIMENT')\n",
    "    print(comprehend_sentiment_response['Sentiment'])\n",
    "comprehend_sentiment_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Sentiment** returned is the result with the highest level of confidence, however within the **SentimentScore** object each of the various sentiments are returned with their corresponding confidence levels.\n",
    "\n",
    "Let's do a little cleanup so we can look at all the data together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|￣￣￣￣￣￣￣￣￣￣￣|  \n",
    "|&nbsp;&nbsp;WHAT DOES THE &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|  \n",
    "|&nbsp;&nbsp;COMPUTER KNOW? &nbsp;&nbsp;|  \n",
    "|＿＿＿＿＿＿＿＿＿＿＿|  \n",
    "(\\\\__/) ||  \n",
    "(•ㅅ•) ||  \n",
    "/ 　 づ  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "META-DATA\n",
      "Subject:  Hey there!\n",
      "Sent:  2020-10-30T09:27:00Z\n",
      "Sender:  David McFadden\n",
      "Recipients:  Kevin Duffy\n",
      "\n",
      "ENTITIES\n",
      "People mentioned in this email:\n",
      "- Kevin\n",
      "- Gillian\n",
      "- Dave   David McFadden\n",
      "\n",
      "Dates mentioned in this email:\n",
      "- next Wednesday at 3pm\n",
      "- Monday\n",
      "\n",
      "Locations mentioned in this email:\n",
      "- 1007 Main Street, Belfast\n",
      "\n",
      "Organisations mentioned in this email:\n",
      "- AWS\n",
      "- Amazon AI Services\n",
      "- Example Corp.\n",
      "- Example Corp\n",
      "\n",
      "SENTIMENT\n",
      "NEUTRAL\n"
     ]
    }
   ],
   "source": [
    "entities = comprehend_entity_response['Entities']  \n",
    "high_conf_entities = list(filter(lambda item: item['Score'] > 0.8, entities))\n",
    "\n",
    "people = list(filter(lambda item: item['Type'] == 'PERSON', high_conf_entities))\n",
    "dates = list(filter(lambda item: item['Type'] == 'DATE', high_conf_entities))\n",
    "locations = list(filter(lambda item: item['Type'] == 'LOCATION', high_conf_entities))\n",
    "organisations = list(filter(lambda item: item['Type'] == 'ORGANIZATION', high_conf_entities))\n",
    "\n",
    "def print_entity_list(list):\n",
    "    for item in list:\n",
    "        text = item['Text'].replace('\\n', ' ')\n",
    "        text = text.strip()\n",
    "        print(f\"- {text}\")\n",
    "        \n",
    "\n",
    "def print_entity_info():\n",
    "    print('ENTITIES')\n",
    "    print('People mentioned in this email:')\n",
    "    print_entity_list(people)\n",
    "    print('')\n",
    "    print('Dates mentioned in this email:')\n",
    "    print_entity_list(dates)\n",
    "    print('')\n",
    "    print('Locations mentioned in this email:')\n",
    "    print_entity_list(locations)\n",
    "    print('')\n",
    "    print('Organisations mentioned in this email:')\n",
    "    print_entity_list(organisations)\n",
    "\n",
    "print_meta_data()\n",
    "print('')\n",
    "print_entity_info()\n",
    "print('')\n",
    "print_sentiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A little Fancier? Thinking about the Conversation..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/dave-email-conversation.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon Lex\n",
    "Amazon Lex is a service for building conversational interfaces. https://aws.amazon.com/lex/\n",
    "\n",
    "<img src=\"img/amazon-lex.png\" width=\"600\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Amazon Lex can recognise Conversational Intents, we can use it as a very quick test to show what might be possible if we were to do some classification work. Here we create a very small model with two Intents - one that can recognise phrases associated with Meeting Requests and one that can identify phrases associated with Deadlines. Since we don't have much data yet we have just added in some initial variations.\n",
    "\n",
    "There is no cost for Model Training or Hosting, we will only pay for requests to the service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/lex-bot.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LEX MODEL\n",
    "\n",
    "**Meeting Intent Phrases:**\n",
    "- I was wondering if you had time to meet with me on  {date}\n",
    "- Can you meet with me on {date}\n",
    "- Could I get some time with you on {date}\n",
    "- Are you available on {date}\n",
    "- are you available sometime {date}\n",
    "- do you have some time to meet with me {date}\n",
    "- Could you do {date}\n",
    "\n",
    "\n",
    "**Deadline Intent Phrases**\n",
    "- I will need any comments back by {date}\n",
    "- The deadline is {date}\n",
    "- This needs to be done for {date}\n",
    "- Final comments are due {date}\n",
    "- This needs completed by {date}\n",
    "- I will need any compliments back by {date}\n",
    "- I will need any questions back by {date}\n",
    "\n",
    "\n",
    "<BR>\n",
    "Each of these uses an Entity type of DATE within the phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how we could use this with our email:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey Kevin!\\n\\xa0\\nHow are you? It’s been a while since we’ve chatted.\\n\\xa0\\nI was wondering if you had time to meet with Gillian and me next Wednesday at 3pm?\\n\\xa0\\nI also was also hoping you could take a look over the following documents:\\nAmazing Flow Charts\\nGetting Fancy with AWS Step Functions\\nPrototyping with Buttery Services\\n\\xa0\\nI’ll need any comments/questions/compliments back by Monday.\\n\\xa0\\nYou can find out more about the Amazon AI Services here.\\n\\xa0\\nCheers,\\nDave\\n\\xa0\\nDavid McFadden |\\xa0Developer\\nExample Corp. \\n1007 Main Street, Belfast\\nTel: +44 2896496000\\xa0\\n\\xa0\\nDisclaimer: The contents of this e-mail and attached files in no way reflect any policies of Example Corp or their affiliated superhero counterparts.\\n\\xa0\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_email_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Roughly break down the email (for our first pass by paragraphs,identified by new lines _\\n_).\n",
    "- Call Amazon Lex for each of those paragraphs. We won't use any of the conversational features - we are just looking for what Intent might be matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import boto3\n",
    "\n",
    "#https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/lex-runtime.html\n",
    "lex = boto3.client('lex-runtime')\n",
    "\n",
    "bot_name = 'analyse_date_phrases'\n",
    "bot_alias = 'demo'\n",
    "lex_array = []\n",
    "\n",
    "#Split by new line and remove blanks\n",
    "paragraphs = parsed_email_text.split('\\n')\n",
    "paragraphs = list(filter(lambda item: (len(item.strip()) > 0), paragraphs))\n",
    "\n",
    "for paragraph in paragraphs:\n",
    "    email_intent = lex.post_text(\n",
    "        botName=bot_name,\n",
    "        botAlias=bot_alias,\n",
    "        userId=str(uuid.uuid4()),\n",
    "        inputText=paragraph\n",
    "    )\n",
    "    email_intent['paragraph'] = paragraph\n",
    "    lex_array.append(email_intent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of what a Lex Response looks like (we added in the paragraph attribute ourselves):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"c36be114-2996-45af-adf0-b4e29f7f2c20\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"x-amzn-requestid\": \"c36be114-2996-45af-adf0-b4e29f7f2c20\",\n",
      "      \"date\": \"Sat, 31 Oct 2020 17:57:33 GMT\",\n",
      "      \"content-type\": \"application/json\",\n",
      "      \"content-length\": \"511\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  },\n",
      "  \"intentName\": \"meeting_request\",\n",
      "  \"nluIntentConfidence\": {\n",
      "    \"score\": 0.9\n",
      "  },\n",
      "  \"alternativeIntents\": [\n",
      "    {\n",
      "      \"intentName\": \"AMAZON.FallbackIntent\",\n",
      "      \"slots\": {}\n",
      "    },\n",
      "    {\n",
      "      \"intentName\": \"deadline\",\n",
      "      \"nluIntentConfidence\": {\n",
      "        \"score\": 0.41\n",
      "      },\n",
      "      \"slots\": {\n",
      "        \"date\": null\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"slots\": {\n",
      "    \"date\": null\n",
      "  },\n",
      "  \"message\": \"What date?\",\n",
      "  \"messageFormat\": \"PlainText\",\n",
      "  \"dialogState\": \"ElicitSlot\",\n",
      "  \"slotToElicit\": \"date\",\n",
      "  \"sessionId\": \"2020-10-31T17:57:33.667Z-nGEHyqIB\",\n",
      "  \"botVersion\": \"11\",\n",
      "  \"paragraph\": \"I was wondering if you had time to meet with Gillian and me next Wednesday at 3pm?\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(lex_array[2], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only pieces are using for this test are the **intentName** (which intent was matched, if any) and **nluIntentConfidence** (the confidence of the accuracy the service has).\n",
    "\n",
    "Let's convert what we have to an easier format to review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score / Type : Text\n",
      "--------------------\n",
      "0.7 / meeting_request : How are you? It’s been a while since we’ve chatted.\n",
      "0.9 / meeting_request : I was wondering if you had time to meet with Gillian and me next Wednesday at 3pm?\n",
      "0.88 / deadline : I’ll need any comments/questions/compliments back by Monday.\n",
      "0.65 / meeting_request : 1007 Main Street, Belfast\n"
     ]
    }
   ],
   "source": [
    "print(f\"Score / Type : Text\")\n",
    "print('--------------------')\n",
    "\n",
    "for item in lex_array:\n",
    "    intent = item.get('intentName', '')\n",
    "    dialogState = item.get('dialogState', '')\n",
    "    confidence = item.get('nluIntentConfidence', '')\n",
    "    \n",
    "    if(len(intent) > 0): \n",
    "        print(str(confidence['score']) +' / '+str(intent) +' : ' +item['paragraph'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see it isn't perfect, and we would definitely have some work to do - but we did find the requests we were interested in, even though the the model was not trained with those exact phrases.\n",
    "\n",
    "Let's filter out lower confidence ones and do some formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REQUESTS\n",
      "Meeting Requests:\n",
      "I was wondering if you had time to meet with Gillian and me next Wednesday at 3pm?\n",
      "\n",
      "Deadline Requests:\n",
      "I’ll need any comments/questions/compliments back by Monday.\n"
     ]
    }
   ],
   "source": [
    "high_conf_intents = list(filter(lambda item: item.get('nluIntentConfidence', {\"score\":0})['score'] >= 0.8, lex_array))\n",
    "meeting_requests = list(filter(lambda item: item.get('intentName', '') == 'meeting_request', high_conf_intents))\n",
    "deadlines = list(filter(lambda item: item.get('intentName', '') == 'deadline', high_conf_intents))\n",
    "\n",
    "def print_intents_found():\n",
    "    print('REQUESTS')\n",
    "    print('Meeting Requests:')\n",
    "    for item in meeting_requests:\n",
    "        print(item['paragraph'])\n",
    "\n",
    "    print('')\n",
    "\n",
    "    print('Deadline Requests:')\n",
    "    for item in deadlines:\n",
    "        print(item['paragraph'])\n",
    "\n",
    "print_intents_found()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|￣￣￣￣￣￣￣￣￣￣￣|  \n",
    "|&nbsp;&nbsp;WHAT DOES THE &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|  \n",
    "|&nbsp;&nbsp;COMPUTER KNOW? &nbsp;&nbsp;|  \n",
    "|＿＿＿＿＿＿＿＿＿＿＿|  \n",
    "(\\\\__/) ||  \n",
    "(•ㅅ•) ||  \n",
    "/ 　 づ  \n",
    "\n",
    "Let's combine it all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "META-DATA\n",
      "Subject:  Hey there!\n",
      "Sent:  2020-10-30T09:27:00Z\n",
      "Sender:  David McFadden\n",
      "Recipients:  Kevin Duffy\n",
      "\n",
      "ENTITIES\n",
      "People mentioned in this email:\n",
      "- Kevin\n",
      "- Gillian\n",
      "- Dave   David McFadden\n",
      "\n",
      "Dates mentioned in this email:\n",
      "- next Wednesday at 3pm\n",
      "- Monday\n",
      "\n",
      "Locations mentioned in this email:\n",
      "- 1007 Main Street, Belfast\n",
      "\n",
      "Organisations mentioned in this email:\n",
      "- AWS\n",
      "- Amazon AI Services\n",
      "- Example Corp.\n",
      "- Example Corp\n",
      "\n",
      "SENTIMENT\n",
      "NEUTRAL\n",
      "\n",
      "REQUESTS\n",
      "Meeting Requests:\n",
      "I was wondering if you had time to meet with Gillian and me next Wednesday at 3pm?\n",
      "\n",
      "Deadline Requests:\n",
      "I’ll need any comments/questions/compliments back by Monday.\n"
     ]
    }
   ],
   "source": [
    "print_meta_data()\n",
    "print('')\n",
    "print_entity_info()\n",
    "print('')\n",
    "print_sentiment()\n",
    "print('')\n",
    "print_intents_found()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/dave-email.png\" width=\"600\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#61A794>QUESTION: What could we do next?</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get a lot more Email Data!\n",
    "- Work on improving how we Pre-Process our Data\n",
    "<br>\n",
    "\n",
    "- Experiment with the other out-of-the box Comprehend Services\n",
    "- Try different variations of our Lex Model and ways of breaking up the text\n",
    "<br>\n",
    "\n",
    "- Do some Custom Entity Recognition or Classification using Comprehend Custom\n",
    "https://docs.aws.amazon.com/comprehend/latest/dg/auto-ml.html\n",
    "(take a look at SageMaker Ground Truth for help with labelling your data https://aws.amazon.com/sagemaker/groundtruth/)\n",
    "<br>\n",
    "\n",
    "- Look at the tools in SageMaker and write some completely custom models... https://aws.amazon.com/sagemaker/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
